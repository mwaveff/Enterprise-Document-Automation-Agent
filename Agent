'''
Enterprise Document Automation Agent
    This enterprise-ready document automation agent ingests complex documents like PDFs and contracts to extract key data, populate a structured database,
    perform risk analysis on terms and penalties, and generate a finalized document or summary report.
Configuration
    Install dependencies: image.png
    If you are using Google ADK components specifically:
    pip install google-adk
    You might also need requests and packages for Jupyter/Kaggle if using them outside this script:
    pip install requests ipython jupyter_server
'''

import os
import time
import json
import datetime
from typing import List, Dict, Any, Optional


import requests
from kaggle_secrets import UserSecretsClient 
from IPython.core.display import display, HTML 
from jupyter_server.serverapp import list_running_servers 


from google.adk.agents import Agent
from google.adk.models.google_llm import Gemini
from google.adk.runners import InMemoryRunner
from google.adk.tools import google_search


from google import genai
from google.genai import types
from google.genai.types import (
    File,
    GenerateContentConfig,
    HttpRetryOptions,
    Schema,
    Type
)

'''
Initialization Block Description
    Authentication: Securely retrieves and sets the Gemini API Key from Kaggle Secrets.
    Configuration: Defines external A2A (Application-to-Application) endpoints and configures an HTTP Retry Policy for reliability.
    Ingestion & Context: Initializes the genai.Client, uploads the contract file to the API, and creates a unique Session ID for the MemorySessionService
    to track context across the entire analysis workflow.
'''

try:
    GOOGLE_API_KEY = UserSecretsClient().get_secret("GOOGLE_API_KEY")
    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
    print("âœ… Gemini API key setup complete")
except ImportError:
    print("âŒ Kaggle Secrets not available. Ensure you're in a Kaggle Notebook")
except KeyError:
    print("âŒ Authentication Error: Add 'GOOGLE_API_KEY' to Kaggle secrets")

EXTERNAL_API_URL = "https://api.external-service.com/process_contract"
API_KEY = "YOUR_SECURE_API_KEY_12345"

retry_config = HttpRetryOptions(
    attempts=5,
    exp_base=2,
    initial_delay=1,
    http_status_codes=[429, 500, 503, 504]
)

client = genai.Client()
file_path = "/kaggle/input/contract/contract01.png"
uploaded_file = None 

try:
    uploaded_file = client.files.upload(file=file_path)
    print(f"âœ… File successfully uploaded: {uploaded_file.name}")
    
    SESSION_ID = uploaded_file.name.split('/')[-1]
    MemorySessionService.create_session(SESSION_ID)
    print(f"âœ… Session created with ID: {SESSION_ID}")

except FileNotFoundError:
    print(f"âŒ Error: File not found at path {file_path}. Check the path.")
    raise
except Exception as e:
    print(f"âŒ An error occurred during file upload or session setup: {e}")

'''
Core Functionality
It uses a static dictionary, _storage, to hold temporary data associated with a unique Session ID (like the ID of the processed document).
Purpose: To manage and share context (intermediate results, statuses, etc.) between the different, separate steps of your document processing workflow.
Key Methods:
    create_session: Starts a new context space.
    save_data / get_data: Stores and retrieves specific pieces of information within that context.
    delete_session: Cleans up the stored data after the workflow is complete.
'''

class MemorySessionService:
    _storage: Dict[str, Dict[str, Any]] = {}

    @classmethod
    def create_session(cls, session_id: str) -> None:
        if session_id not in cls._storage:
            cls._storage[session_id] = {"created_at": time.time(), "data": {}}

    @classmethod
    def save_data(cls, session_id: str, key: str, value: Any) -> None:
        if session_id in cls._storage:
            cls._storage[session_id]["data"][key] = value

    @classmethod
    def get_data(cls, session_id: str, key: str) -> Optional[Any]:
        return cls._storage.get(session_id, {}).get("data", {}).get(key)

    @classmethod
    def delete_session(cls, session_id: str) -> None:
        cls._storage.pop(session_id, None)

    @classmethod
    def list_sessions(cls) -> Dict[str, Any]:
        return cls._storage

print("âœ… MemorySessionService loaded successfully")

'''
The OUTPUT_SCHEMA
    The OUTPUT_SCHEMA (using the GenAI SDK's Schema objects) acts as a strict contract that compels the Large Language Model (LLM) to return its entire analysis in
    a predictable, nested JSON format. 
    This is crucial for down-stream processing (like the populate_database and A2A functions).
'''

OUTPUT_SCHEMA = Schema(
    type=Type.OBJECT,
    properties={
        "DocumentMetadata": Schema(
            type=Type.OBJECT, 
            properties={
                "DocumentID": Schema(type=Type.STRING, description="Auto-generated document ID"),
                "DocumentType": Schema(type=Type.STRING, description="Contract type classification"),
                "ProcessingDate": Schema(type=Type.STRING, description="ISO timestamp of analysis")
            },
            required=["DocumentID", "DocumentType"]
        ),
        "RiskAnalysisModule": Schema(
            type=Type.OBJECT, 
            properties={
                "OverallRiskScore": Schema(
                    type=Type.STRING, 
                    enum=["LOW", "MEDIUM", "HIGH", "CRITICAL"],
                    description="Final risk assessment score"
                ),
                "RiskRationale": Schema(type=Type.STRING, description="Explanation of risk score"),
                "KeyRiskClauses": Schema(
                    type=Type.ARRAY, 
                    items=Schema(type=Type.STRING),
                    description="List of identified high-risk clauses"
                ),
                "ComplianceStatus": Schema(type=Type.STRING, description="Overall compliance status")
            },
            required=["OverallRiskScore", "RiskRationale", "KeyRiskClauses"]
        ),
        "SummaryReport": Schema(
            type=Type.STRING, 
            description="Comprehensive narrative summary from STAGE 4"
        ),
        "ExtractedEntities": Schema(
            type=Type.OBJECT,
            properties={
                "Parties": Schema(type=Type.ARRAY, items=Schema(type=Type.STRING)),
                "EffectiveDate": Schema(type=Type.STRING),
                "KeyTerms": Schema(type=Type.ARRAY, items=Schema(type=Type.STRING))
            }
        )
    },
    required=["DocumentMetadata", "RiskAnalysisModule", "SummaryReport"]
)

prompt_text = (
    "Execute a four-stage workflow for end-to-end document automation on the provided contract file, ensuring "
    "to analyze terms and penalties and perform a real-time risk assessment. "
    "The final output must be the structured JSON data and a Summary Report adhering to the following workflow:\n\n"
    "**STAGE 1: Document Ingestion and Pre-processing**..."
)

print("âœ… OutputShema loaded successfully")

'''
Function mock_external_api_call
    The function is a placeholder designed to simulate the behavior of a real, external A2A (Application-to-Application) API service. 
    It allows the core workflow to be tested and validated without actually sending data to a live, production endpoint.
'''

def mock_external_api_call(data: Dict[str, Any]) -> Dict[str, Any]:
    print(f"[{time.strftime('%H:%M:%S')}] ğŸ”„ A2A: External service received request.")
    document_id = data.get("DocumentMetadata", {}).get("DocumentID", "N/A")
    risk_score = data.get("RiskAnalysisModule", {}).get("OverallRiskScore", "UNKNOWN")
    
    time.sleep(1) 
    
    response_data = {
        "status": "SUCCESS",
        "message": f"Document {document_id} processed successfully.",
        "processed_risk": risk_score,
        "external_ref_id": f"EXT-{int(time.time())}"
    }
    return response_data

print("âœ… Mock_external_api_call loaded successfully")

'''
Function send_data_to_external_system
    This function implements the Application-to-Application (A2A) protocol within your bot, acting as the final data transfer step after the contract analysis is complete. 
    Its purpose is to prepare the analyzed JSON data and send it to an external system (like an archiving database or an ERP system).
'''

def send_data_to_external_system(json_data: str) -> str:
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}",
        "X-App-Source": "ContractAutomationBot"
    }
    
    try:
        data = json.loads(json_data)
        external_result = mock_external_api_call(data)
        
        if external_result.get("status") == "SUCCESS":
            return f"A2A SUCCESS: External service confirmed processing. Ref ID: {external_result['external_ref_id']}"
        else:
            return f"A2A ERROR: External service returned an error: {external_result.get('message', 'Unknown error')}"
            
    except json.JSONDecodeError:
        return "A2A ERROR: Invalid JSON for sending."
    except Exception as e:
        return f"A2A INTERNAL ERROR: {str(e)}"

print("âœ… Send_data_to_external_system loaded successfully")

'''
Function perform_risk_assessment
    The function serves as a rule-based expert system that automatically scans a list of extracted contract clauses for predefined legal risk keywords. 
    It returns a structured JSON object containing the overall risk level and a rationale.
    This function is designed to be called by the Large Language Model (LLM) as a tool during the analysis phase to provide an objective, immediate risk rating.
'''

def perform_risk_assessment(extracted_clauses: List[str]) -> str:
    if not extracted_clauses:
        return json.dumps({
            "risk_score": "LOW", 
            "rationale": "No clauses provided for analysis"
        })
    
    try:
        high_risk_keywords = ["NOTWITHSTANDING CLIENT'S OWN NEGLIGENCE", "PENALTY EXCEEDS 10%"]
        medium_risk_keywords = ["LIMITATION OF LIABILITY", "INDEMNIFICATION"]
        
        risk_level = "LOW"
        found_risks = []
        
        for clause in extracted_clauses:
            clause_upper = clause.upper()
            if any(keyword in clause_upper for keyword in high_risk_keywords):
                risk_level = "CRITICAL"
                found_risks.append(clause[:100] + "...")
            elif risk_level != "CRITICAL" and any(keyword in clause_upper for keyword in medium_risk_keywords):
                risk_level = "MEDIUM"
                found_risks.append(clause[:100] + "...")
        
        rationale = (
            f"Critical risk clauses found: {found_risks}" 
            if risk_level == "CRITICAL" 
            else "Standard risk assessment completed"
        )
        
        return json.dumps({
            "risk_score": risk_level, 
            "rationale": rationale,
            "analyzed_clauses_count": len(extracted_clauses)
        })
        
    except Exception as e:
        return json.dumps({
            "risk_score": "UNKNOWN", 
            "rationale": f"Risk assessment error: {str(e)}"
        })

print("âœ… Perform_risk_assessment loaded successfully")

'''
Function populate_database
Core Functionality:
    Input Parsing and Validation
    Data Enrichment and Simulation
    A2A Integration
    Status Reporting
'''

def populate_database(json_data: str) -> str:
    try:
        data = json.loads(json_data)
        
        required_fields = ["DocumentMetadata", "RiskAnalysisModule"]
        for field in required_fields:
            if field not in data:
                return f"ERROR: Missing required field '{field}'"
        
        doc_id = data["DocumentMetadata"].get("DocumentID", "N/A")
        risk_score = data["RiskAnalysisModule"].get("OverallRiskScore", "UNKNOWN")
        
        print(f"ğŸ“Š Inserting document {doc_id} with risk score: {risk_score}")
        
        data["ProcessingTimestamp"] = datetime.datetime.now().isoformat()
        
        print(f"â¡ï¸ Starting A2A transfer for document {doc_id}...")
        a2a_result = send_data_to_external_system(json.dumps(data))
        print(f"ğŸ‰ A2A Status: {a2a_result}")

        return f"SUCCESS: Record {doc_id} saved. Risk: {risk_score}. A2A Result: {a2a_result}"
        
    except json.JSONDecodeError as e:
        return f"ERROR: Invalid JSON - {str(e)}"
    except Exception as e:
        return f"ERROR: Database operation failed - {str(e)}"

print("âœ… Populate_database loaded successfully")

'''
Multi-Stage Workflow Execution
Stage 1: Document Processing and Tool Use:
    Model Call: The client.models.generate_content is called with the model (gemini-2.0-flash), the prompt_text, and the uploaded_file reference.
    Tool Integration: The model is configured with two tools (perform_risk_assessment, populate_database). This allows the model to internally call the perform_risk_assessment function to get a real-time, rule-based risk score while generating the overall analysis.
    Context Preservation: The textual analysis result (response1.text) is immediately saved to the MemorySessionService using the unique SESSION_ID. This ensures the result is persisted for the next stage.
    Delay: A time.sleep(5) pause is added, often used in multi-stage workflows to allow time for the API to fully process and log the intermediate result.
Stage 2: JSON Conversion and Finalization:
    Retrieval: The raw analysis is retrieved from memory using MemorySessionService.get_data().
    Conversion Prompt: A new prompt (conversion_prompt) is generated, instructing the model to take the retrieved stage1_text and convert it strictly according to the defined OUTPUT_SCHEMA.
    Structured Output.
    Parsing and Display.
    Error Reporting.
    Final Status.
Cleanup (finally Block):
    Session Deletion.
    File Deletion.
    Graceful Exit.
'''

try:
    print("ğŸ”„ Stage 1: Starting Document Automation Agent with functions")
    print(f"ğŸ“ Prompt length: {len(prompt_text)}")
    
    response1 = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[prompt_text, uploaded_file],
        config=GenerateContentConfig(
            tools=[perform_risk_assessment, populate_database]
        )
    )
    
    print("âœ… Stage 1 completed successfully!")
    
    if hasattr(response1, 'text'):
        MemorySessionService.save_data(SESSION_ID, "stage1_analysis_text", response1.text)
    print(f"ğŸ“„ Response1 text length: {len(response1.text)}")

    time.sleep(5) 
    
    print("ğŸ”„ Stage 2: Converting result to JSON")

    stage1_text = MemorySessionService.get_data(SESSION_ID, "stage1_analysis_text") or 'No analysis available'
    print(f"ğŸ§  Retrieved Stage 1 text (length: {len(stage1_text)}) from session {SESSION_ID}")

    conversion_prompt = f"""
    Convert the following contract analysis to strict JSON format according to the schema:

    CONTRACT ANALYSIS:
    {stage1_text}

    JSON REQUIREMENTS:
    ...
    """
    
    response2 = client.models.generate_content(
        model="gemini-2.0-flash", 
        contents=conversion_prompt,
        config=GenerateContentConfig(
            response_mime_type="application/json",
            response_schema=OUTPUT_SCHEMA
        )
    )
    
    print("âœ… Stage 2 completed")
    
    print(f"ğŸ“Š Response2 type: {type(response2)}")
    if hasattr(response2, 'text'):
        print(f"ğŸ“„ Response2 text length: {len(response2.text)}")
    else:
        print("âŒ Response2 has no 'text' attribute")
        print(f"ğŸ“‹ Response2 attributes: {dir(response2)}")
    
    print("\nâœ… ANALYSIS COMPLETED")
    
    if response2 and hasattr(response2, 'text') and response2.text:
        print("\nğŸ¯ CONTRACT ANALYSIS RESULTS:")
        print("=" * 50)
        
        try:
            result = json.loads(response2.text)
            
            metadata = result.get('DocumentMetadata', {})
            risk_module = result.get('RiskAnalysisModule', {})
            
            print(f"ğŸ“„ Document type: {metadata.get('DocumentType', 'Unknown')}")
            print(f"ğŸ†” Document ID: {metadata.get('DocumentID', 'None')}")
            print(f"ğŸ“… Processed: {metadata.get('ProcessingDate', 'Unknown')}")
            print()
            
            risk_score = risk_module.get('OverallRiskScore', 'UNKNOWN')
            risk_color = {
                'LOW': 'ğŸŸ¢', 
                'MEDIUM': 'ğŸŸ¡', 
                'HIGH': 'ğŸŸ ', 
                'CRITICAL': 'ğŸ”´'
            }.get(risk_score, 'âšª')
            
            print(f"{risk_color} Risk level: {risk_score}")
            print(f"ğŸ“ Rationale: {risk_module.get('RiskRationale', 'None')}")
            print(f"ğŸ›ï¸ Compliance status: {risk_module.get('ComplianceStatus', 'Unknown')}")
            print()
            
            risk_clauses = risk_module.get('KeyRiskClauses', [])
            if risk_clauses:
                print(f"ğŸ” Risk clauses identified: {len(risk_clauses)}")
                for i, clause in enumerate(risk_clauses, 1):
                    print(f"    {i}. {clause}")
            else:
                print("ğŸ” No risk clauses identified")
            print()

            entities = result.get('ExtractedEntities', {})
            if entities:
                print("ğŸ“‹ Extracted data:")
                print(f"    ğŸ¤ Parties: {', '.join(entities.get('Parties', []))}")
                print(f"    ğŸ“… Effective date: {entities.get('EffectiveDate', 'Unknown')}")
                
                key_terms = entities.get('KeyTerms', [])
                if key_terms:
                    print(f"    ğŸ“ Key terms: {len(key_terms)} found")
                    for i, term in enumerate(key_terms[:2], 1):
                        print(f"      {i}. {term[:80]}...")
            
            print("\n" + "=" * 50)
            print("âœ… Analysis successfully completed! Contract is ready for further review")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print("ğŸ“‹ Raw result:")
            print(response2.text)
    else:
        print("âŒ Failed to get analysis result")
        if response2:
            print(f"ğŸ“‹ Response2 content: {response2}")
        
    if response2 and hasattr(response2, 'text') and response2.text:
        print("\nJSON result:")
        print(response2.text)
    else:
        print("\nâŒ No JSON result available")
        
except Exception as e:
    print(f"\nâŒ Error during analysis: {e}")
    import traceback
    print(f"ğŸ” Full error traceback:\n{traceback.format_exc()}")
    
finally:
    try:
        if 'SESSION_ID' in locals():
            MemorySessionService.delete_session(SESSION_ID)
            print(f"âœ… Session deleted: {SESSION_ID}")
        if 'uploaded_file' in locals():
            client.files.delete(name=uploaded_file.name)
            print(f"âœ… File deleted: {uploaded_file.name}")
    except Exception as e:
        print(f"âš ï¸ File already deleted or not uploaded: {e}")
